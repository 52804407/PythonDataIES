{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10 \n",
    "by Martin Hronec\n",
    "\n",
    "Contents:\n",
    "\n",
    "1. [Decorators](10_decorators.ipynb)\n",
    "2. [Testing](#Testing)\n",
    "\n",
    "(3.) [Notes on efficient computing](10_efficient_computing.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "* many ways to test code\n",
    "* you've all done an exploratory/manual testing\n",
    "* to cover the whole codebase with manual tests, it is necessary:\n",
    "    * list all the code/projects features\n",
    "    * collect all (different) types of inputs it \n",
    "    * collect the corresponding expected results\n",
    "* !problem: change in code => change the above\n",
    "    * not fun => **automated testing**\n",
    "        * running test from script instead of manually\n",
    "   \n",
    "* 2 main test categories:\n",
    "    * integration tests - testing multiple if multiple components work together\n",
    "    * unit tests - testing a single component\n",
    "\n",
    "* (most) functional tests consist of:\n",
    "    1. **Arrange** - conditions in/for which we test\n",
    "    2. **Act** - running the behaviour we want to test\n",
    "    3. **Assert** - check if behaviour produced expected result\n",
    "    4. **Cleanup** - don't influence other tests\n",
    "\n",
    "* the most basic test can be done using `assert` method\n",
    "    * e.g. lets check/test if `len` method is the same as `__len__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = [1,2,3,5] \n",
    "assert len(a_list) == a_list.__len__(), \"Function len returned differnt result than method __len__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we could try different data-structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tuple = (1,2,3,5)\n",
    "assert len(a_tuple) == a_tuple.__len__(), \"Function len returned differnt result than method __len__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tuple = (1,2,3,5)\n",
    "assert len(a_tuple) == a_tuple.__len__(), \"Function len returned differnt result than method __len__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "2. Your result is off.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MARTIN~1\\AppData\\Local\\Temp/ipykernel_2724/2568627209.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2. Your result is off.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: 2. Your result is off."
     ]
    }
   ],
   "source": [
    "assert sum([1,1]) == 3, '2. Your result is off.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* instead of testing on the REPL, we can put our tests into a test script and run it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_1.py\n"
     ]
    }
   ],
   "source": [
    "# %load test_1.py\n",
    "def test_sum():\n",
    "    assert sum([1,1]) == 2, \"Should be 2\"\n",
    "    \n",
    "def test_len_vs__len__():\n",
    "    a_tuple = (1,2,3,5)\n",
    "    assert len(a_tuple) == a_tuple.__len__(), \"Function len returned differnt result than method __len__\"\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test_sum()\n",
    "    test_len_vs__len__()\n",
    "    print('All tests passed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "%run test_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* OK for simple check, cumbersome for more tests\n",
    "    * => **test runners**\n",
    "* test runner = application designed for running tests\n",
    "    * check the output\n",
    "    * offer tools for diagnosing\n",
    "    \n",
    "* many test runners available for Python\n",
    "    * *unittest* (built into the Python standard library)\n",
    "    * nose/nose2\n",
    "    * doctest\n",
    "    * robot\n",
    "    * **pytest**, ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a framework for building simple and scalable tests\n",
    "* one of the most popular Python testing frameworks\n",
    "    * feature-rich\n",
    "    * a lot of available [plugins](https://docs.pytest.org/en/latest/reference/plugin_list.html)\n",
    " \n",
    "* pytest works with the simple assert statements\n",
    "    * not necessarily the case with other test runners\n",
    "\n",
    "* how does pytest know which tests to run?\n",
    "    * by default it runs all files of the form `test_*.py` or `*_test.py` in the current directory and subdirectories\n",
    "        * however check [conventions for test discovery rules](https://docs.pytest.org/en/6.2.x/goodpractices.html#test-discovery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.7.3, pytest-6.0.1, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: C:\\Users\\Martin Hronec\\Projects\\phd\\teaching\\DPP_IES\\10_testing, configfile: pytest.ini\n",
      "collected 15 items\n",
      "\n",
      "test_2.py F.                                                             [ 13%]\n",
      "test_naive.py ..                                                         [ 26%]\n",
      "tests\\test_3.py F.                                                       [ 40%]\n",
      "tests\\test_fixture_smtp.py .                                             [ 46%]\n",
      "tests\\test_fixtures_data.py F                                            [ 53%]\n",
      "tests\\test_mark_example.py ..                                            [ 66%]\n",
      "tests\\test_parametrize_example.py ..F..                                  [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "__________________________________ test_sum ___________________________________\n",
      "\n",
      "    def test_sum():\n",
      ">       assert sum([1,1]) == 3, \"Should be 2\"\n",
      "E       AssertionError: Should be 2\n",
      "E       assert 2 == 3\n",
      "E        +  where 2 = sum([1, 1])\n",
      "\n",
      "test_2.py:3: AssertionError\n",
      "__________________________________ test_sum ___________________________________\n",
      "\n",
      "    def test_sum():\n",
      ">       assert sum([1,1]) == 3, \"Should be 2\"\n",
      "E       AssertionError: Should be 2\n",
      "E       assert 2 == 3\n",
      "E        +  where 2 = sum([1, 1])\n",
      "\n",
      "tests\\test_3.py:3: AssertionError\n",
      "_______________________________ test_addressing _______________________________\n",
      "\n",
      "data_names =   Title   Surname     Addressing\n",
      "0  Mgr.   Kalerab   Mgr. Kalerab\n",
      "1  Ing.  Mrkvicka  Ing. Mrkvicka\n",
      "2   NaN   Slanina        Slanina\n",
      "\n",
      "    def test_addressing(data_names):\n",
      "        df = data_names\n",
      "        titles = df['Title']\n",
      "        surnames = df['Surname']\n",
      "        expected = df['Addressing']\n",
      ">       assert (titles + ' ' + expected == surnames).all()\n",
      "E       assert False\n",
      "E        +  where False = <bound method Series.all of 0    False\\n1    False\\n2    False\\ndtype: bool>()\n",
      "E        +    where <bound method Series.all of 0    False\\n1    False\\n2    False\\ndtype: bool> = 0     Mgr. Mg...\\ndtype: object == 0     Kalerab... dtype: object\n",
      "E             Use -v to get the full diff.all\n",
      "\n",
      "tests\\test_fixtures_data.py:14: AssertionError\n",
      "______________________________ test_eval[6*9-42] ______________________________\n",
      "\n",
      "test_input = '6*9', expected = 42\n",
      "\n",
      "    @pytest.mark.parametrize(\"test_input,expected\", [(\"3+5\", 8), (\"2+4\", 6), (\"6*9\", 42)])\n",
      "    def test_eval(test_input, expected):\n",
      ">       assert eval(test_input) == expected\n",
      "E       AssertionError: assert 54 == 42\n",
      "E        +  where 54 = eval('6*9')\n",
      "\n",
      "tests\\test_parametrize_example.py:32: AssertionError\n",
      "=========================== short test summary info ===========================\n",
      "FAILED test_2.py::test_sum - AssertionError: Should be 2\n",
      "FAILED tests/test_3.py::test_sum - AssertionError: Should be 2\n",
      "FAILED tests/test_fixtures_data.py::test_addressing - assert False\n",
      "FAILED tests/test_parametrize_example.py::test_eval[6*9-42] - AssertionError:...\n",
      "======================== 4 failed, 11 passed in 1.48s =========================\n"
     ]
    }
   ],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* what does it tell us:\n",
    "    * the system tests are run on (Python, pytest version, and any pluggins\n",
    "    * *rootdir* : where are we running things from\n",
    "    * [XX%] next to each test script shows success rate of all tests\n",
    "    * it will show you a failure report with detailed explanation (not here)\n",
    "        * lets fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile test_2.py\n",
    "#%%read test_2.py\n",
    "\n",
    "def test_sum():\n",
    "    assert sum([1,1]) == 3, \"Should be 2\"\n",
    "\n",
    "def test_len_vs__len__():\n",
    "    a_tuple = (1,2,3,5)\n",
    "    assert len(a_tuple) == a_tuple.__len__(), \"Function len returned differnt result than method __len__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.7.3, pytest-6.0.1, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: C:\\Users\\Martin Hronec\\Projects\\phd\\teaching\\DPP_IES\\10_testing, configfile: pytest.ini\n",
      "collected 15 items\n",
      "\n",
      "test_2.py F.                                                             [ 13%]\n",
      "test_naive.py ..                                                         [ 26%]\n",
      "tests\\test_3.py F.                                                       [ 40%]\n",
      "tests\\test_fixture_smtp.py .                                             [ 46%]\n",
      "tests\\test_fixtures_data.py F                                            [ 53%]\n",
      "tests\\test_mark_example.py ..                                            [ 66%]\n",
      "tests\\test_parametrize_example.py ..F..                                  [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "__________________________________ test_sum ___________________________________\n",
      "\n",
      "    def test_sum():\n",
      ">       assert sum([1,1]) == 3, \"Should be 2\"\n",
      "E       AssertionError: Should be 2\n",
      "E       assert 2 == 3\n",
      "E        +  where 2 = sum([1, 1])\n",
      "\n",
      "test_2.py:3: AssertionError\n",
      "__________________________________ test_sum ___________________________________\n",
      "\n",
      "    def test_sum():\n",
      ">       assert sum([1,1]) == 3, \"Should be 2\"\n",
      "E       AssertionError: Should be 2\n",
      "E       assert 2 == 3\n",
      "E        +  where 2 = sum([1, 1])\n",
      "\n",
      "tests\\test_3.py:3: AssertionError\n",
      "_______________________________ test_addressing _______________________________\n",
      "\n",
      "data_names =   Title   Surname     Addressing\n",
      "0  Mgr.   Kalerab   Mgr. Kalerab\n",
      "1  Ing.  Mrkvicka  Ing. Mrkvicka\n",
      "2   NaN   Slanina        Slanina\n",
      "\n",
      "    def test_addressing(data_names):\n",
      "        df = data_names\n",
      "        titles = df['Title']\n",
      "        surnames = df['Surname']\n",
      "        expected = df['Addressing']\n",
      ">       assert (titles + ' ' + expected == surnames).all()\n",
      "E       assert False\n",
      "E        +  where False = <bound method Series.all of 0    False\\n1    False\\n2    False\\ndtype: bool>()\n",
      "E        +    where <bound method Series.all of 0    False\\n1    False\\n2    False\\ndtype: bool> = 0     Mgr. Mg...\\ndtype: object == 0     Kalerab... dtype: object\n",
      "E             Use -v to get the full diff.all\n",
      "\n",
      "tests\\test_fixtures_data.py:14: AssertionError\n",
      "______________________________ test_eval[6*9-42] ______________________________\n",
      "\n",
      "test_input = '6*9', expected = 42\n",
      "\n",
      "    @pytest.mark.parametrize(\"test_input,expected\", [(\"3+5\", 8), (\"2+4\", 6), (\"6*9\", 42)])\n",
      "    def test_eval(test_input, expected):\n",
      ">       assert eval(test_input) == expected\n",
      "E       AssertionError: assert 54 == 42\n",
      "E        +  where 54 = eval('6*9')\n",
      "\n",
      "tests\\test_parametrize_example.py:32: AssertionError\n",
      "=========================== short test summary info ===========================\n",
      "FAILED test_2.py::test_sum - AssertionError: Should be 2\n",
      "FAILED tests/test_3.py::test_sum - AssertionError: Should be 2\n",
      "FAILED tests/test_fixtures_data.py::test_addressing - assert False\n",
      "FAILED tests/test_parametrize_example.py::test_eval[6*9-42] - AssertionError:...\n",
      "======================== 4 failed, 11 passed in 0.84s =========================\n"
     ]
    }
   ],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* output next to the script indecates the status of each test:\n",
    "    * \".\" - test passed\n",
    "    * \"F\" - test failed\n",
    "    * \"E\" - test raised an unexcpected exception\n",
    "\n",
    "* it does not only show you the AssertionError though\n",
    "    * what does it show us (compared to the simple assert statement)?\n",
    "\n",
    "* if we want to run only some tests, we can specify which to ignore\n",
    "    * `--ignore`\n",
    "    * `--ignore-glob` - using glob (wildcard like patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/test_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/test_3.py\n",
    "\n",
    "def test_sum():\n",
    "    assert sum([1,1]) == 3, \"Should be 2\"\n",
    "\n",
    "def test_len_vs__len__():\n",
    "    a_tuple = (1,2,3,5)\n",
    "    assert len(a_tuple) == a_tuple.__len__(), \"Function len returned differnt result than method __len__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pcz02m4h\\Projects\\DPP_IES\\10_testing\n"
     ]
    }
   ],
   "source": [
    "# checking where we are\n",
    "!cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.9.4, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: C:\\Users\\pcz02m4h\\Projects\\DPP_IES\\10_testing\n",
      "plugins: anyio-2.2.0\n",
      "collected 4 items\n",
      "\n",
      "test_1.py ..                                                             [ 50%]\n",
      "test_2.py F.                                                             [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "__________________________________ test_sum ___________________________________\n",
      "\n",
      "    def test_sum():\n",
      ">       assert sum([1,1]) == 3, \"Should be 2\"\n",
      "E       AssertionError: Should be 2\n",
      "E       assert 2 == 3\n",
      "E        +  where 2 = sum([1, 1])\n",
      "\n",
      "test_2.py:3: AssertionError\n",
      "=========================== short test summary info ===========================\n",
      "FAILED test_2.py::test_sum - AssertionError: Should be 2\n",
      "========================= 1 failed, 3 passed in 0.11s =========================\n"
     ]
    }
   ],
   "source": [
    "!pytest --ignore=tests/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.9.4, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: C:\\Users\\pcz02m4h\\Projects\\DPP_IES\\10_testing\n",
      "plugins: anyio-2.2.0\n",
      "collected 5 items\n",
      "\n",
      "test_1.py ..                                                             [ 40%]\n",
      "test_2.py F.                                                             [ 80%]\n",
      "tests\\test_not_to_run.py .                                               [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "__________________________________ test_sum ___________________________________\n",
      "\n",
      "    def test_sum():\n",
      ">       assert sum([1,1]) == 3, \"Should be 2\"\n",
      "E       AssertionError: Should be 2\n",
      "E       assert 2 == 3\n",
      "E        +  where 2 = sum([1, 1])\n",
      "\n",
      "test_2.py:3: AssertionError\n",
      "=========================== short test summary info ===========================\n",
      "FAILED test_2.py::test_sum - AssertionError: Should be 2\n",
      "========================= 1 failed, 4 passed in 0.11s =========================\n"
     ]
    }
   ],
   "source": [
    "# when not ignoring\n",
    "!pytest --ignore-glob=*_3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* in most modern code editors, managing a set of tests is more user friendly than from command line\n",
    "\n",
    "\n",
    "* tests often depend on:\n",
    "    * data\n",
    "    * test doubles\n",
    "* we don't want to mess with the originals => pytest **fixtures**\n",
    "\n",
    "### Fixtures\n",
    "* \"arranging\" part of the test\n",
    "\n",
    "* a method for providing:\n",
    "    * data\n",
    "    * test doubles\n",
    "    * state setup \n",
    "\n",
    "* more tests using the same underlying dataset -> use fixture\n",
    "     * (repeating) data provided by a single function [decorated](#Decorators) with `@pytest.fixture`\n",
    "     \n",
    "* test depending on a fixture needs to have a fixture as an argument\n",
    "\n",
    "* let's look at the test double first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pcz02m4h\\Projects\\DPP_IES\\10_testing\n"
     ]
    }
   ],
   "source": [
    "!cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load test_fixture_smtp.py\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def smtp():\n",
    "    \"\"\"Initialize and return SMTP client session object\"\"\"\n",
    "    import smtplib\n",
    "    return smtplib.SMTP(\"smtp.gmail.com\")\n",
    "\n",
    "def test_ehlo(smtp):\n",
    "    \"\"\"Test response from sending Extended Helo (EHLO) is 250.\"\"\"\n",
    "    response, msg = smtp.ehlo()\n",
    "    assert response == 250\n",
    "    # assert 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.9.4, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: C:\\Users\\pcz02m4h\\Projects\\DPP_IES\\10_testing\n",
      "plugins: anyio-2.2.0\n",
      "collected 1 item\n",
      "\n",
      "test_fixture_smtp.py .                                                   [100%]\n",
      "\n",
      "============================== 1 passed in 0.16s ==============================\n"
     ]
    }
   ],
   "source": [
    "!pytest test_fixture_smtp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* now fixture for providing data\n",
    "    * note: when providing path, think about the sourcedirectory! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tests/test_fixtures_data.py\n",
    "import pytest \n",
    "\n",
    "@pytest.fixture\n",
    "def data_names():\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('data/test_data_names.csv')\n",
    "    return df\n",
    "\n",
    "def test_addressing(data_names):\n",
    "    df = data_names\n",
    "    titles = df['Title']\n",
    "    surnames = df['Surname']\n",
    "    expected = df[['Addressing']]\n",
    "    assert (titles + ' ' + expected == surnames).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.9.4, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: C:\\Users\\pcz02m4h\\Projects\\DPP_IES\\10_testing\n",
      "plugins: anyio-2.2.0\n",
      "collected 1 item\n",
      "\n",
      "tests\\test_fixtures_data.py F                                            [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "_______________________________ test_addressing _______________________________\n",
      "\n",
      "data_names =   Title   Surname     Addressing\n",
      "0  Mgr.   Kalerab   Mgr. Kalerab\n",
      "1  Ing.  Mrkvicka  Ing. Mrkvicka\n",
      "2   NaN   Slanina        Slanina\n",
      "\n",
      "    def test_addressing(data_names):\n",
      "        df = data_names\n",
      "        titles = df['Title']\n",
      "        surnames = df['Surname']\n",
      "        expected = df['Addressing']\n",
      ">       assert (titles + ' ' + expected == surnames).all()\n",
      "E       assert False\n",
      "E        +  where False = <bound method NDFrame._add_numeric_operations.<locals>.all of 0    False\\n1    False\\n2    False\\ndtype: bool>()\n",
      "E        +    where <bound method NDFrame._add_numeric_operations.<locals>.all of 0    False\\n1    False\\n2    False\\ndtype: bool> = 0     Mgr. Mg...\\ndtype: object == 0     Kalerab... dtype: object\n",
      "E             Use -v to get the full diff.all\n",
      "\n",
      "tests\\test_fixtures_data.py:14: AssertionError\n",
      "=========================== short test summary info ===========================\n",
      "FAILED tests/test_fixtures_data.py::test_addressing - assert False\n",
      "============================== 1 failed in 0.53s ==============================\n"
     ]
    }
   ],
   "source": [
    "!pytest tests/test_fixtures_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* when to avoid fixtures:\n",
    "    * using fixtures fixtures is as bas as using tests redundantly\n",
    "    *  => **marks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marks - test filtering\n",
    "\n",
    "* you might want to only run couple of your tests\n",
    "    * full suite of tests only sometimes\n",
    "    \n",
    "* to filter which tests to run:\n",
    "    * name-based filtering\n",
    "    * directory scoping \n",
    "    * **test categorization** (`-m` parameter)\n",
    "    \n",
    "* create **marks** (custom labels) to label any test you like (can have multiple labels)\n",
    "    * e.g. you can categorize your tests by dependencies (e.g. access to database - could be `@pytest.mark.database_access`\n",
    "* to run only tests in specific category (mark) `pytest -m <mark>`\n",
    "* to *not* run tests with specific mark `pytest -m \"not <mark>\"`\n",
    "\n",
    "* you should also [register the custom markers](https://stackoverflow.com/questions/60806473/pytestunknownmarkwarning-unknown-pytest-mark-xxx-is-this-a-typo) in *pytest.ini* file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tests/test_mark_example.py\n",
    "import pytest \n",
    "\n",
    "@pytest.mark.database\n",
    "def test_pg_read():\n",
    "    pass\n",
    "\n",
    "@pytest.mark.database\n",
    "def test_pg_write():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -m database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* there are few marks out of the box:\n",
    "    * **skip** skips a test unconditionally\n",
    "    * **skipif** skips a test if the expression passed to it evaluates to True\n",
    "    * **parametrize** creates multiple variants of a test with different values as arguments\n",
    "    \n",
    "* you can see a list of all the marks pytest knows about by running `pytest --markers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --markers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test parametrization\n",
    "\n",
    "* using only slightly different input and output would lead to repeating test definitions\n",
    "    * DRY!\n",
    "* fixtures not very good with only slightly different inputs and expected outputs\n",
    "    * **parametrize** a single test definition a get variants of the test for you with the parameters you specify\n",
    "    * mind the syntax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tests/test_parametrize_example.py\n",
    "import pytest\n",
    "import unicodedata\n",
    "\n",
    "#######\n",
    "# Function we would like to test should be defined in package code, not here.\n",
    "########\n",
    "def drop_diacritics(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Strip accents from input String.\n",
    "    \n",
    "    :param text: The input string.\n",
    "    :returns: The processed string.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        raise TypeError(f'Input text should be a string, not %s', type(text))\n",
    "    \n",
    "    # Return the normal form for the Unicode string\n",
    "    # 'NFKD' stands for the normal form KD  \n",
    "    text = unicodedata.normalize('NFKD',text)\n",
    "    output = ''\n",
    "    \n",
    "    for char in text:\n",
    "        if not unicodedata.combining(char):\n",
    "            output += char\n",
    "            \n",
    "    return output\n",
    "#### \n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"test_input,expected\", [(\"3+5\", 8), (\"2+4\", 6), (\"6*9\", 42)])\n",
    "def test_eval(test_input, expected):\n",
    "    assert eval(test_input) == expected\n",
    "    \n",
    "@pytest.mark.parametrize(\n",
    "    'original,output',\n",
    "    [\n",
    "        ('řeřicha', 'rericha'),\n",
    "        ('čeština', 'cestina')\n",
    "    ]\n",
    ") \n",
    "def test_drop_diacritics(original:str, output:str) -> None:\n",
    "    assert drop_diacritics(original) == output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.9.4, pytest-6.2.3, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: C:\\Users\\pcz02m4h\\Projects\\DPP_IES\\10_testing, configfile: pytest.ini\n",
      "plugins: anyio-2.2.0\n",
      "collected 5 items\n",
      "\n",
      "tests\\test_parametrize_example.py ..F..                                  [100%]\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "______________________________ test_eval[6*9-42] ______________________________\n",
      "\n",
      "test_input = '6*9', expected = 42\n",
      "\n",
      "    @pytest.mark.parametrize(\"test_input,expected\", [(\"3+5\", 8), (\"2+4\", 6), (\"6*9\", 42)])\n",
      "    def test_eval(test_input, expected):\n",
      ">       assert eval(test_input) == expected\n",
      "E       AssertionError: assert 54 == 42\n",
      "E        +  where 54 = eval('6*9')\n",
      "\n",
      "tests\\test_parametrize_example.py:32: AssertionError\n",
      "=========================== short test summary info ===========================\n",
      "FAILED tests/test_parametrize_example.py::test_eval[6*9-42] - AssertionError:...\n",
      "========================= 1 failed, 4 passed in 0.10s =========================\n"
     ]
    }
   ],
   "source": [
    "!pytest tests/test_parametrize_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing features to explore\n",
    "\n",
    "* [plugins](https://docs.pytest.org/en/latest/reference/plugin_list.html)\n",
    "    * requests-mock\n",
    "    * database-mock\n",
    "\n",
    "* [CI/CD](https://docs.github.com/en/actions/guides/about-continuous-integration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
